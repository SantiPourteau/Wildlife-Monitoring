{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_Weights\n",
    "from dataset_loader.dataset import DatasetConfig, ObjectDetectionDataset  # Asegúrate de que estén en el mismo directorio o en el PATH\n",
    "from torchvision.ops import box_iou\n",
    "from torchsummary import summary\n",
    "from torchinfo import summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Configuración del dispositivo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando {device}\")\n",
    "\n",
    "# Configuración del Dataset\n",
    "dataset_config = DatasetConfig(\n",
    "    img_width=1792, \n",
    "    img_height=1434,\n",
    "    dataset_path=\"../../data_maskrcnn/dataset_hst\",  # Cambia esta ruta por la de tu dataset\n",
    "    splits=[\"train_data\", \"val_data\"]\n",
    ")\n",
    "\n",
    "# Crear DataLoaders para entrenamiento y validación\n",
    "train_dataset = ObjectDetectionDataset(config=dataset_config, split=\"train_data\", is_train=True)\n",
    "val_dataset = ObjectDetectionDataset(config=dataset_config, split=\"val_data\", is_train=False)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=4, \n",
    "    shuffle=True, \n",
    "    collate_fn=lambda x: tuple(zip(*x))  # Agrupa las imágenes y etiquetas en listas\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=4, \n",
    "    shuffle=False, \n",
    "    collate_fn=lambda x: tuple(zip(*x))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pesos personalizados cargados con éxito.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\santi\\AppData\\Local\\Temp\\ipykernel_287420\\906123833.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"custom_maskrcnn.pt\"))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cargar modelo Mask R-CNN solo para detección de objetos\n",
    "weights = MaskRCNN_ResNet50_FPN_Weights.DEFAULT\n",
    "model = maskrcnn_resnet50_fpn(weights=weights)\n",
    "model.roi_heads.mask_predictor = None  # Remover la cabeza de predicción de máscaras\n",
    "\n",
    "# Número de clases (incluye la clase fondo)\n",
    "num_classes = 4  # 3 clases más la clase fondo\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "# Ajustar el umbral de puntuación\n",
    "model.roi_heads.score_thresh = 0.5\n",
    "# print(model.roi_heads.score_thresh)\n",
    "\n",
    "# Cargar pesos personalizados si corresponde\n",
    "try:\n",
    "    model.load_state_dict(torch.load(\"custom_maskrcnn.pt\"))\n",
    "    print(\"Pesos personalizados cargados con éxito.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"No se encontró el archivo de pesos personalizados. Usando pesos preentrenados.\")\n",
    "\n",
    "model.to(device)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(targets)\n",
    "\n",
    "# model(images, targets)  # Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary(model, input_size=(1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import box_iou\n",
    "\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    iou_thresholds = torch.linspace(0.5, 0.95, steps=10).to(device)\n",
    "    metrics = {\"mAP\": 0.0, \"mAP_50\": 0.0}\n",
    "    mAP_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in data_loader:\n",
    "            # Mover imágenes al dispositivo\n",
    "            images = [img.to(device) for img in images]\n",
    "\n",
    "            # Obtener predicciones del modelo\n",
    "            outputs = model(images)\n",
    "\n",
    "            for output, target in zip(outputs, targets):\n",
    "                # Manejo de predicciones vacías\n",
    "                if len(output[\"boxes\"]) == 0:\n",
    "                    continue\n",
    "\n",
    "                # Mover predicciones y targets al dispositivo\n",
    "                pred_boxes = output[\"boxes\"].to(device)\n",
    "                pred_labels = output[\"labels\"].to(device)\n",
    "                true_boxes = target[\"boxes\"].to(device)\n",
    "                true_labels = target[\"labels\"].to(device)\n",
    "\n",
    "                # Calcular IoUs\n",
    "                ious = box_iou(pred_boxes, true_boxes)\n",
    "                if ious is None:\n",
    "                    print('ious is None')\n",
    "                # print(ious)\n",
    "                # # Calcular mAP para cada umbral de IoU\n",
    "                # for iou_threshold in iou_thresholds:\n",
    "                #     if ious.numel() > 0:  # Evitar casos donde no hay IoUs calculables\n",
    "                #         mAP_scores.append((ious > iou_threshold).float().mean().item())\n",
    "\n",
    "                if ious.numel() > 0:  # Evita problemas con batches sin predicciones\n",
    "                    metrics[\"mAP_50\"] += (ious > 0.5).float().mean().item() \n",
    "\n",
    "\n",
    "        # Normalizar métricas\n",
    "        if len(data_loader.dataset) > 0:\n",
    "            metrics[\"mAP_50\"] /= len(data_loader.dataset)\n",
    "            # metrics[\"mAP\"] = sum(mAP_scores) / len(iou_thresholds)\n",
    "    print(metrics)\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    # param.requires_grad = False\n",
    "    param.requires_grad = True\n",
    "    \n",
    "# for name, param in model.named_parameters():\n",
    "#     if \"roi_heads.box_predictor.cls_score\" in name or \"roi_heads.box_predictor.bbox_pred\" in name:\n",
    "#         param.requires_grad = True\n",
    "#     elif \"roi_heads.box_head\" in name:\n",
    "#         param.requires_grad = True\n",
    "#     elif \"rpn.head\" in name:\n",
    "#         param.requires_grad = True\n",
    "#     # elif \"backbone.fpn.layer_blocks.3.0\" in name:\n",
    "#     #     param.requires_grad = True\n",
    "#     # elif \"backbone.body.layer4\" in name:\n",
    "#     #     param.requires_grad = True\n",
    "#     else:\n",
    "#         param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backbone.body.conv1.weight True\n",
      "backbone.body.layer1.0.conv1.weight True\n",
      "backbone.body.layer1.0.conv2.weight True\n",
      "backbone.body.layer1.0.conv3.weight True\n",
      "backbone.body.layer1.0.downsample.0.weight True\n",
      "backbone.body.layer1.1.conv1.weight True\n",
      "backbone.body.layer1.1.conv2.weight True\n",
      "backbone.body.layer1.1.conv3.weight True\n",
      "backbone.body.layer1.2.conv1.weight True\n",
      "backbone.body.layer1.2.conv2.weight True\n",
      "backbone.body.layer1.2.conv3.weight True\n",
      "backbone.body.layer2.0.conv1.weight True\n",
      "backbone.body.layer2.0.conv2.weight True\n",
      "backbone.body.layer2.0.conv3.weight True\n",
      "backbone.body.layer2.0.downsample.0.weight True\n",
      "backbone.body.layer2.1.conv1.weight True\n",
      "backbone.body.layer2.1.conv2.weight True\n",
      "backbone.body.layer2.1.conv3.weight True\n",
      "backbone.body.layer2.2.conv1.weight True\n",
      "backbone.body.layer2.2.conv2.weight True\n",
      "backbone.body.layer2.2.conv3.weight True\n",
      "backbone.body.layer2.3.conv1.weight True\n",
      "backbone.body.layer2.3.conv2.weight True\n",
      "backbone.body.layer2.3.conv3.weight True\n",
      "backbone.body.layer3.0.conv1.weight True\n",
      "backbone.body.layer3.0.conv2.weight True\n",
      "backbone.body.layer3.0.conv3.weight True\n",
      "backbone.body.layer3.0.downsample.0.weight True\n",
      "backbone.body.layer3.1.conv1.weight True\n",
      "backbone.body.layer3.1.conv2.weight True\n",
      "backbone.body.layer3.1.conv3.weight True\n",
      "backbone.body.layer3.2.conv1.weight True\n",
      "backbone.body.layer3.2.conv2.weight True\n",
      "backbone.body.layer3.2.conv3.weight True\n",
      "backbone.body.layer3.3.conv1.weight True\n",
      "backbone.body.layer3.3.conv2.weight True\n",
      "backbone.body.layer3.3.conv3.weight True\n",
      "backbone.body.layer3.4.conv1.weight True\n",
      "backbone.body.layer3.4.conv2.weight True\n",
      "backbone.body.layer3.4.conv3.weight True\n",
      "backbone.body.layer3.5.conv1.weight True\n",
      "backbone.body.layer3.5.conv2.weight True\n",
      "backbone.body.layer3.5.conv3.weight True\n",
      "backbone.body.layer4.0.conv1.weight True\n",
      "backbone.body.layer4.0.conv2.weight True\n",
      "backbone.body.layer4.0.conv3.weight True\n",
      "backbone.body.layer4.0.downsample.0.weight True\n",
      "backbone.body.layer4.1.conv1.weight True\n",
      "backbone.body.layer4.1.conv2.weight True\n",
      "backbone.body.layer4.1.conv3.weight True\n",
      "backbone.body.layer4.2.conv1.weight True\n",
      "backbone.body.layer4.2.conv2.weight True\n",
      "backbone.body.layer4.2.conv3.weight True\n",
      "backbone.fpn.inner_blocks.0.0.weight True\n",
      "backbone.fpn.inner_blocks.0.0.bias True\n",
      "backbone.fpn.inner_blocks.1.0.weight True\n",
      "backbone.fpn.inner_blocks.1.0.bias True\n",
      "backbone.fpn.inner_blocks.2.0.weight True\n",
      "backbone.fpn.inner_blocks.2.0.bias True\n",
      "backbone.fpn.inner_blocks.3.0.weight True\n",
      "backbone.fpn.inner_blocks.3.0.bias True\n",
      "backbone.fpn.layer_blocks.0.0.weight True\n",
      "backbone.fpn.layer_blocks.0.0.bias True\n",
      "backbone.fpn.layer_blocks.1.0.weight True\n",
      "backbone.fpn.layer_blocks.1.0.bias True\n",
      "backbone.fpn.layer_blocks.2.0.weight True\n",
      "backbone.fpn.layer_blocks.2.0.bias True\n",
      "backbone.fpn.layer_blocks.3.0.weight True\n",
      "backbone.fpn.layer_blocks.3.0.bias True\n",
      "rpn.head.conv.0.0.weight True\n",
      "rpn.head.conv.0.0.bias True\n",
      "rpn.head.cls_logits.weight True\n",
      "rpn.head.cls_logits.bias True\n",
      "rpn.head.bbox_pred.weight True\n",
      "rpn.head.bbox_pred.bias True\n",
      "roi_heads.box_head.fc6.weight True\n",
      "roi_heads.box_head.fc6.bias True\n",
      "roi_heads.box_head.fc7.weight True\n",
      "roi_heads.box_head.fc7.bias True\n",
      "roi_heads.box_predictor.cls_score.weight True\n",
      "roi_heads.box_predictor.cls_score.bias True\n",
      "roi_heads.box_predictor.bbox_pred.weight True\n",
      "roi_heads.box_predictor.bbox_pred.bias True\n",
      "roi_heads.mask_head.0.0.weight True\n",
      "roi_heads.mask_head.0.0.bias True\n",
      "roi_heads.mask_head.1.0.weight True\n",
      "roi_heads.mask_head.1.0.bias True\n",
      "roi_heads.mask_head.2.0.weight True\n",
      "roi_heads.mask_head.2.0.bias True\n",
      "roi_heads.mask_head.3.0.weight True\n",
      "roi_heads.mask_head.3.0.bias True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD([param for param in model.parameters() if param.requires_grad], \n",
    "                            lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.27755348773106286}\n",
      "Loss: 20.2228\n",
      "Validation mAP@50: 0.2776\n",
      "Epoch 2/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.3486024886369705}\n",
      "Loss: 15.5540\n",
      "Validation mAP@50: 0.3486\n",
      "Epoch 3/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.34751553250395734}\n",
      "Loss: 12.7187\n",
      "Validation mAP@50: 0.3475\n",
      "Epoch 4/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.35645273068676825}\n",
      "Loss: 10.7222\n",
      "Validation mAP@50: 0.3565\n",
      "Epoch 5/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.42239475898120715}\n",
      "Loss: 10.4252\n",
      "Validation mAP@50: 0.4224\n",
      "Epoch 6/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4136991060298422}\n",
      "Loss: 10.2020\n",
      "Validation mAP@50: 0.4137\n",
      "Epoch 7/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.41369910732559534}\n",
      "Loss: 10.1657\n",
      "Validation mAP@50: 0.4137\n",
      "Epoch 8/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.41369910732559534}\n",
      "Loss: 9.9799\n",
      "Validation mAP@50: 0.4137\n",
      "Epoch 9/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 10.0055\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 10/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 10.0969\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 11/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 9.9423\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 12/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 10.0007\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 13/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 10.0121\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 14/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 9.9610\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 15/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 9.9670\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 16/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 10.1096\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 17/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 10.0795\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 18/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 9.9965\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 19/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 9.9082\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 20/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 10.1504\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 21/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 9.9858\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 22/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 9.8822\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 23/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 10.0405\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 24/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 10.0952\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 25/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 9.9929\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 26/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 9.8897\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 27/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 9.9057\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 28/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 9.9836\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 29/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 9.9412\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 30/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 10.1052\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 31/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 10.0510\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 32/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 9.8908\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 33/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 10.0216\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 34/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 10.0245\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 35/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 10.0181\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 36/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 10.0086\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 37/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 9.9047\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 38/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 10.0754\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 39/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 9.9742\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 40/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 10.0381\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 41/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 10.0144\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 42/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 9.9657\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 43/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 9.9860\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 44/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 9.9520\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 45/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 9.9523\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 46/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 9.9495\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 47/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 9.9455\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 48/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 9.9439\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 49/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 9.9711\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 50/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 9.9495\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 51/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 9.9056\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 52/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 9.8941\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 53/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 9.9491\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 54/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 10.0505\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 55/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 9.9635\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 56/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 10.0684\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 57/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 10.0990\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 58/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 9.9408\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 59/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 9.9334\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 60/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 9.9773\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 61/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 10.0228\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 62/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 10.0668\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 63/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 9.9694\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 64/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 9.9140\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 65/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 10.0523\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 66/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 9.9920\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 67/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 9.9415\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 68/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 10.0274\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 69/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 10.0162\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 70/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 10.0505\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 71/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 9.9259\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 72/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 10.0576\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 73/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 9.9012\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 74/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 10.2127\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 75/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 9.9557\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 76/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 10.0632\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 77/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 10.0930\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 78/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 9.9524\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 79/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 9.9625\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 80/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 9.9658\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 81/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 9.9816\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 82/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 9.8818\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 83/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 10.1253\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 84/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 9.9577\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 85/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 10.0395\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 86/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 9.9246\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 87/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 10.0511\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 88/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 10.1075\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 89/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 9.9903\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 90/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 10.0844\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 91/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 10.0074\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 92/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 9.9783\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 93/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 9.9097\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 94/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 9.8882\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 95/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 9.9481\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 96/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 9.9591\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 97/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 10.1948\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 98/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 9.9313\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 99/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 9.9772\n",
      "Validation mAP@50: 0.4173\n",
      "Epoch 100/100\n",
      "{'mAP': 0.0, 'mAP_50': 0.4173222961633102}\n",
      "Loss: 9.9809\n",
      "Validation mAP@50: 0.4173\n",
      "Modelo guardado en 'retrained_maskrcnn.pt'\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    for images, targets in train_loader:\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # Adelanto y cálculo de pérdidas\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for key, loss in loss_dict.items() if key != \"loss_mask\")\n",
    "        epoch_loss += losses.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        # print(f\"Loss: {losses.item()}\")\n",
    "\n",
    "    # Actualización del scheduler\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    # Evaluación\n",
    "    metrics = evaluate_model(model, val_loader, device)\n",
    "\n",
    "    # print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    print(f\"Loss: {epoch_loss:.4f}\")\n",
    "    print(f\"Validation mAP@50: {metrics['mAP_50']:.4f}\")\n",
    "    # print(f\"Validation mAP: {metrics['mAP']}\")\n",
    "\n",
    "# # Guardar el modelo reentrenado\n",
    "torch.save(model.state_dict(), \"retrained_maskrcnn:map0.4.pt\")\n",
    "print(\"Modelo guardado en 'retrained_maskrcnn.pt'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 44\u001b[0m\n\u001b[0;32m     42\u001b[0m random_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6\u001b[39m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Plot first image with predicted bounding boxes\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m plot_image_with_boxes(\u001b[43mimages\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrandom_index\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mcpu(), outputs[random_index])\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#do a forward pass and plot images with predicted bounding boxes\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "\n",
    "def plot_image_with_boxes(img, target):\n",
    "    img = img.mul(255).permute(1, 2, 0).byte().numpy()\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.imshow(img)\n",
    "\n",
    "    for box, label in zip(target[\"boxes\"], target[\"labels\"]):\n",
    "        box = box.int().cpu().numpy()\n",
    "        x_min ,y_min, x_max, y_max = box\n",
    "        #desnormalizar\n",
    "        img_width, img_height = img.shape[1], img.shape[0]\n",
    "        # print(img_width, img_height)\n",
    "        # x_min = x_min * img_width\n",
    "        # x_max = x_max * img_width\n",
    "        # y_min = y_min * img_height\n",
    "        # y_max = y_max * img_height\n",
    "\n",
    "        #print predictions\n",
    "        print(f\"Predicción: {label.item()}\")\n",
    "        plt.gca().add_patch(plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, fill=False, edgecolor=\"r\", lw=2))\n",
    "        #add label\n",
    "        plt.text(x_min, y_min, f\"Clase: {label.item()}\", color=\"r\", fontsize=12)\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# Get a batch of images from the dataloader\n",
    "images, targets = next(iter(train_loader))\n",
    "images = [img.to(device) for img in images]\n",
    "\n",
    "# Forward pass\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(images)\n",
    "\n",
    "#print bounding boxes\n",
    "random_index = 6\n",
    "# Plot first image with predicted bounding boxes\n",
    "plot_image_with_boxes(images[random_index].cpu(), outputs[random_index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
